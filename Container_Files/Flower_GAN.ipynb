{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_GAN.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uElkVCfCa8w1",
        "colab_type": "code",
        "outputId": "d249f250-d347-48d4-f000-d6124d575dfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from tensorflow.keras import layers\n",
        "import glob\n",
        "import os\n",
        "import time\n",
        "import tensorflow_datasets as tfds\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wq75A5t68GOt",
        "colab_type": "code",
        "outputId": "ec996858-1a64-4ccd-9f65-d73d416832fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-fDmAn3CgEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = 1020\n",
        "BATCH_SIZE = 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WzOFH1DbESM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tfds.load(\"oxford_flowers102\", split=tfds.Split.TRAIN, batch_size=-1)\n",
        "numpy_ds = tfds.as_numpy(train_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hkhahjydFN8",
        "colab_type": "code",
        "outputId": "0ff5bfc2-de2d-4a49-e22a-fbcd3a6a7b14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_images_np = numpy_ds[\"image\"]\n",
        "print(train_images_np.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9-WArKUfWX8",
        "colab_type": "code",
        "outputId": "41141116-b931-4a10-b12d-be7fdd167404",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_images = []\n",
        "for i in train_images_np:\n",
        "  train_images.append(cv2.resize(i, (128,128))/255)\n",
        "\n",
        "train_images = np.array(train_images, dtype='float32')\n",
        "print(train_images.shape)\n",
        "train_images_np = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOqnK7WtwsQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_generator():\n",
        "  model = tf.keras.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(32 * 32 * 256, use_bias=False, input_shape=(100,)))\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Reshape((32,32,256)))\n",
        "\n",
        "  #Conv2DTranspose blocks to create the images\n",
        "  model.add(layers.Conv2DTranspose(128, (5,5), strides=(1,1), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 32, 32, 128)\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(64, (5,5), strides=(2,2), padding='same', use_bias=False))\n",
        "  assert model.output_shape == (None, 64, 64, 64)\n",
        "  model.add(layers.BatchNormalization())\n",
        "  model.add(layers.LeakyReLU())\n",
        "\n",
        "  model.add(layers.Conv2DTranspose(3, (5,5), strides=(2,2), padding='same', use_bias=False, activation='tanh'))\n",
        "  assert model.output_shape == (None, 128, 128, 3)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQbWf_VQxbGR",
        "colab_type": "code",
        "outputId": "97d8a54e-658b-4888-9af3-a0fd8d7aedb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "source": [
        "#Test generator while untrained\n",
        "generator = make_generator()\n",
        "noise = tf.random.normal([1,100])\n",
        "generated_image = generator(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0])\n",
        "plt.show() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtCpL-DY8_pX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_discriminator():\n",
        "    model = tf.keras.Sequential()\n",
        "\n",
        "    model.add(layers.Conv2D(64, (5,5), strides=(2,2), padding='same', input_shape=[128,128,3]))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5,5), strides=(2,2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(256, (5,5), strides=(2,2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2hneF9Q9UMF",
        "colab_type": "code",
        "outputId": "174532f0-c616-4584-fd27-543129151a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#test the untrained discriminator\n",
        "discriminator = make_discriminator()\n",
        "decision = discriminator(generated_image)\n",
        "print(decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYP68mDM9giL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEXawfIh9oy2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7WDdAx89rSa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZpOqO2t944D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3Hcqyk3hVs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = 'drive/My Drive/Flower_GAN/training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AvnQNxp97yT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 10000\n",
        "noise_dim = 100\n",
        "num_example_to_generate = 16\n",
        "\n",
        "#Will reuse this seed overtime\n",
        "seed = tf.random.normal([num_example_to_generate, noise_dim])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nR5de80-WUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "  noise = tf.random.normal(([BATCH_SIZE, noise_dim]))\n",
        "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "    #Generate the images from noise with training mode = True\n",
        "    generated_images = generator(noise, training=True)\n",
        "\n",
        "    real_output = discriminator(images, training=True)\n",
        "    fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "    gen_loss = generator_loss(fake_output)\n",
        "    disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "  gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "  generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBjk9vomAFZP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    if (epoch + 1) % 1000 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "\n",
        "    \n",
        "    print(\"Time for epoch {} was {} seconds\".format(epoch+1, time.time() - start))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLTji-QaBT1C",
        "colab_type": "code",
        "outputId": "d32aec50-c6f7-472f-eca1-4f00d503fe5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "print(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MfVPsfJHJIo",
        "colab_type": "code",
        "outputId": "d9f33e41-0feb-4519-bea7-398efe80176a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train(dataset, EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMLuxoQ6IDBc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#get a generated image\n",
        "generated_test = generator(seed, training=False)\n",
        "plt.imshow(generated_test[0, :, :, 0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OkcFSMpfLl0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(train_images[0])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xBEcZvFh4zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}